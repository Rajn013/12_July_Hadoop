{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f28f4e7f",
   "metadata": {},
   "source": [
    "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c3f65",
   "metadata": {},
   "source": [
    "Use the ElementTree module to parse the core-site.xml configuration file.\n",
    "Extract the core components' names and their corresponding values from the XML.\n",
    "Display the core components of Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ba93ae",
   "metadata": {},
   "source": [
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea548827",
   "metadata": {},
   "source": [
    "Use the hdfs library in Python to interact with HDFS.\n",
    "Connect to the HDFS cluster using the Namenode's address and port.\n",
    "Use the hdfs.client.Client object to get the status of files in the specified directory.\n",
    "Sum up the sizes of all the files to calculate the total file size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314623b7",
   "metadata": {},
   "source": [
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e59d4",
   "metadata": {},
   "source": [
    "Implement a MapReduce function to split the text file into words and count their occurrences.\n",
    "Use the mrjob library in Python to define the MapReduce job.\n",
    "Run the job on the Hadoop cluster and retrieve the results to display the top N most frequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a770d24",
   "metadata": {},
   "source": [
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb221e6f",
   "metadata": {},
   "source": [
    "Use the requests library in Python to make HTTP requests to the Hadoop NameNode and DataNode REST APIs.\n",
    "Retrieve the health status information from the responses and display the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e9971",
   "metadata": {},
   "source": [
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1f80e",
   "metadata": {},
   "source": [
    "Use the hdfs library in Python to interact with HDFS.\n",
    "Connect to the HDFS cluster using the Namenode's address and port.\n",
    "Use the hdfs.client.Client object to list all files and directories in the specified path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9034ab",
   "metadata": {},
   "source": [
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291ac07",
   "metadata": {},
   "source": [
    "Use the hdfs library in Python to interact with HDFS.\n",
    "Connect to the HDFS cluster using the Namenode's address and port.\n",
    "Use the hdfs.client.Client object to get the status of all DataNodes.\n",
    "Analyze the storage capacities of each DataNode and identify the nodes with the highest and lowest capacities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38944c83",
   "metadata": {},
   "source": [
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec678139",
   "metadata": {},
   "source": [
    "Use the requests library in Python to make HTTP requests to YARN's ResourceManager REST API.\n",
    "Submit a Hadoop job to the cluster.\n",
    "Periodically check the job status and progress using the ResourceManager API.\n",
    "Retrieve the final output of the job once it is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d8670",
   "metadata": {},
   "source": [
    "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9d841",
   "metadata": {},
   "source": [
    "Use the requests library in Python to make HTTP requests to YARN's ResourceManager REST API.\n",
    "Submit a Hadoop job to the cluster with specified resource requirements (e.g., memory, CPU, etc.).\n",
    "Monitor the resource usage of the job during its execution using the ResourceManager API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0215b57",
   "metadata": {},
   "source": [
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a9db1",
   "metadata": {},
   "source": [
    "Create multiple copies of the same input data with different split sizes.\n",
    "Run the same MapReduce job with each input split size on the Hadoop cluster.\n",
    "Measure and compare the execution time of each job to analyze the impact of input split size on performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
